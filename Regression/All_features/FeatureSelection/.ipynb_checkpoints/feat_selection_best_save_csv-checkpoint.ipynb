{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78a776-b242-465f-874c-76468872a55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS\n",
      "../TrainingResults/norm/WS\n",
      "../TrainingResults/norm/best_modelsWS_info.csv\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Define the data columns\n",
    "data_columns = ['Provincial_Class','Federal_Class','Regime','Vegetation_Type','Vegetation_Cover','Woody_Canopy_Cover','Moss_Cover','Phragmites','Soil_Type',\n",
    "'Surface_Water_Present','Saturation_Depth','Living_Moss_Depth','Organic_Depth','Hydrogeomorphic_Class',\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "feature_selection_techniques = {\n",
    "    \"SelectKBest_f_regression\": SelectKBest(score_func=f_regression),\n",
    "    \"SelectKBest_mutual_info_regression\": SelectKBest(score_func=mutual_info_regression),\n",
    "    \"VarianceThreshold\": VarianceThreshold(threshold=0.1),  # Example threshold, adjust as needed\n",
    "}\n",
    "\n",
    "# Define a mapping from model names to model classes\n",
    "model_mapping = {\n",
    "    'Ridge': Ridge,\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor,\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor,\n",
    "    'RandomForestRegressor': RandomForestRegressor,\n",
    "    'AdaBoostRegressor': AdaBoostRegressor,\n",
    "    'KNeighborsRegressor': KNeighborsRegressor,\n",
    "    'MLPRegressor': MLPRegressor,\n",
    "    'ElasticNet': ElasticNet,\n",
    "    'SGDRegressor': SGDRegressor,\n",
    "    'SVR': SVR,\n",
    "    'BayesianRidge': BayesianRidge,\n",
    "    'KernelRidge': KernelRidge,\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'RANSACRegressor': RANSACRegressor,\n",
    "    'TheilSenRegressor': TheilSenRegressor\n",
    "}\n",
    "\n",
    "RND=42 \n",
    "# Function to load, evaluate, and retrain the best model with feature selection\n",
    "def load_evaluate_and_retrain_best_model_with_feature_selection(csv_file, model_name, model_path, hyperparameters, results_column):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data.sort_values(by='id').reset_index(drop=True)\n",
    "    data = data.drop(columns=['id'])\n",
    "    #print(data)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_column]\n",
    "\n",
    "    # Split the data into training and testing sets with a fixed random state\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=RND)\n",
    "    #print(X_test)\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Load the original model and calculate MSE\n",
    "    if model_name == 'TensorFlow':\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred = y_pred.ravel()\n",
    "    else:\n",
    "        model = joblib.load(model_path)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    mse_original = mean_squared_error(y_test, y_pred)\n",
    "    selection_results = {'Original': mse_original}\n",
    "    feature_selection_data = []  # To store feature selection results\n",
    "    test_results = []  # To store test results (actual and predicted)\n",
    "\n",
    "    # Iterate over each feature selection technique\n",
    "    for name, selector in feature_selection_techniques.items():\n",
    "        mse_values = []\n",
    "        for k in range(2, len(data_columns) + 1):  # Iterate over all possible numbers of features\n",
    "            if isinstance(selector, SelectKBest):\n",
    "                selector.set_params(k=k)\n",
    "                X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "                X_test_selected = selector.transform(X_test)\n",
    "                selected_features = selector.get_support(indices=True)[:k]  # Get the selected features for KBest\n",
    "            elif isinstance(selector, VarianceThreshold):\n",
    "                # Apply the threshold once\n",
    "                X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "                X_test_selected = selector.transform(X_test)\n",
    "                if X_train_selected.shape[1] < k:\n",
    "                    # If fewer features than k are selected, skip\n",
    "                    continue\n",
    "                else:\n",
    "                    # Otherwise, slice the selected features\n",
    "                    X_train_selected = X_train_selected[:, :k]\n",
    "                    X_test_selected = X_test_selected[:, :k]\n",
    "                    selected_features = np.argsort(-selector.variances_)[:k]  # Get the top k features by variance\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Retrain the model with the selected features\n",
    "            if model_name == 'TensorFlow':\n",
    "                # Model setup and training for TensorFlow\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.InputLayer(input_shape=(X_train_selected.shape[1],)),\n",
    "                    tf.keras.layers.Dense(hyperparameters['units'], activation=hyperparameters['activation']),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "                              loss='mse')\n",
    "                model.fit(X_train_selected, y_train, epochs=hyperparameters['epochs'], verbose=0)\n",
    "                y_pred = model.predict(X_test_selected)\n",
    "                y_pred = y_pred.ravel()\n",
    "            else:\n",
    "                # Model setup and training for other models\n",
    "                model_class = model_mapping.get(model_name)\n",
    "                if not model_class:\n",
    "                    print(f\"Unknown model name: {model_name}\")\n",
    "                    continue\n",
    "\n",
    "                model_hyperparameters = {k.split('__', 1)[1]: v for k, v in hyperparameters.items() if\n",
    "                                         k.startswith(model_name.lower())}\n",
    "                model = model_class(**model_hyperparameters)\n",
    "                model.fit(X_train_selected, y_train)\n",
    "                y_pred = model.predict(X_test_selected)\n",
    "\n",
    "            # Calculate MSE and store the results\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse_values.append(mse)\n",
    "\n",
    "            # Store feature selection results\n",
    "            feature_selection_data.append({\n",
    "                'CSV File': csv_file,\n",
    "                'Model': model_name,\n",
    "                'Selection Method': name,\n",
    "                'Number of Features': k,\n",
    "                'MSE': mse,\n",
    "                'Selected Features': [data_columns[i] for i in selected_features]\n",
    "            })\n",
    "\n",
    "            # Store test results\n",
    "            test_results.append({\n",
    "                'CSV File': csv_file,\n",
    "                'Model': model_name,\n",
    "                'Selection Method': name,\n",
    "                'Number of Features': k,\n",
    "                'Actual': ','.join(map(str, [\"{:.5f}\".format(val) for val in y_test.values])),\n",
    "                'Predicted': ','.join(map(str, [\"{:.5f}\".format(val) for val in y_pred])),\n",
    "                'MSE': mse,\n",
    "                'Selected Features': [data_columns[i] for i in selected_features]\n",
    "            })\n",
    "        selection_results[name] = mse_values\n",
    "\n",
    "    return selection_results, feature_selection_data, test_results\n",
    "\n",
    "# Main loop for processing each results column\n",
    "result_columns_list = ['WS', 'NR', 'PR', 'SR', 'SFST', 'WS_Benefit', 'NR_Benefit', 'PR_Benefit', 'SR_Benefit', 'SFST_Benefit']  # Specify your actual target column names here\n",
    "\n",
    "# Function to convert a list of values to a comma-separated string\n",
    "def list_to_string(values):\n",
    "    return ','.join(map(str, values))\n",
    "\n",
    "# Define a list to hold the results from both norm and non_norm directories\n",
    "all_results = []\n",
    "\n",
    "for result_column in result_columns_list:\n",
    "    print(result_column)\n",
    "    for norm_type in ['norm','non_norm']:\n",
    "        model_directory = f\"../TrainingResults/{norm_type}/{result_column}\"\n",
    "        best_models_file = f\"../TrainingResults/{norm_type}/best_models{result_column}_info.csv\"\n",
    "        print(model_directory)\n",
    "        print(best_models_file)\n",
    "\n",
    "        if not os.path.exists(best_models_file):\n",
    "            continue\n",
    "\n",
    "        best_models_df = pd.read_csv(best_models_file)\n",
    "\n",
    "        if best_models_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Find the best model from best_models_df\n",
    "        best_model = best_models_df.loc[best_models_df['rmse'].idxmin()]  # Assuming there's an 'mse' column\n",
    "\n",
    "        # Load data for the best model\n",
    "        if norm_type=='norm':\n",
    "            csv_file = os.path.join('../../../Data_ML/4_out_csvs_regression_norm', best_model['csv_file'])\n",
    "        elif norm_type=='non_norm':\n",
    "            csv_file = os.path.join('../../../Data_ML/4_out_csvs_regression', best_model['csv_file'])\n",
    "\n",
    "        \n",
    "        model_name = best_model['model_name']\n",
    "        model_path = os.path.join(model_directory,\n",
    "                                  f\"{best_model['csv_file']}_{model_name}_model.pkl\" if model_name != 'TensorFlow' else f\"{best_model['csv_file']}_TensorFlow_model.h5\")\n",
    "        hyperparameters = ast.literal_eval(best_model['hyperparameters'])\n",
    "\n",
    "        # Perform feature selection and retraining for the best model\n",
    "        reduction_results, feature_selection_data, test_results = load_evaluate_and_retrain_best_model_with_feature_selection(csv_file, model_name, model_path, hyperparameters, result_column)\n",
    "\n",
    "        # Plot MSE values for each reduction method for the current CSV file\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for name, mse_values in reduction_results.items():\n",
    "            if name != 'Original':  # Skip 'Original' since it doesn't have varying components\n",
    "                # Clip the MSE values at 4\n",
    "                clipped_mse_values = [min(mse, 3) for mse in mse_values]\n",
    "                plt.plot(range(2, len(clipped_mse_values) + 2), clipped_mse_values, label=name)\n",
    "\n",
    "                # Find the index of the minimum MSE value (from clipped values)\n",
    "                min_index = np.argmin(clipped_mse_values)\n",
    "                min_mse = clipped_mse_values[min_index]\n",
    "                # Plot the black dot for the minimum MSE value\n",
    "                plt.plot(min_index + 2, min_mse, 'ko')  # 'ko' is for black dot\n",
    "                # Annotate the minimum MSE value\n",
    "                plt.annotate(f'{min_mse:.4f}', (min_index + 2, min_mse),\n",
    "                             textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "        # Add original MSE to the plot, clipped if necessary\n",
    "        original_clipped = min(reduction_results['Original'], 4)\n",
    "        plt.axhline(y=original_clipped, color='gray', linestyle='--', label='Original')\n",
    "\n",
    "        # Add labels and legend\n",
    "        plt.xlabel('Number of Selected Features')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.title(f'MSE with Feature Selection for Model: {model_name} {result_column} ({best_model[\"csv_file\"]})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.ylim(0, 4)  # Set the y-axis to display up to MSE of 4\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"feature_selection_{result_column}_{norm_type}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Process and save test results\n",
    "        test_results_df = pd.DataFrame(test_results)\n",
    "        test_results_df.to_csv(f\"test_results_{result_column}_{norm_type}.csv\", index=False)\n",
    "\n",
    "        # Append results to the global list for final analysis\n",
    "        all_results.extend(feature_selection_data)\n",
    "\n",
    "# Final analysis can be done here with `all_results`\n",
    "# Save `all_results` if needed\n",
    "final_results_df = pd.DataFrame(all_results)\n",
    "final_results_df.to_csv(\"final_feature_selection_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc2acb-54cf-4f23-81a2-c3c1b2a77bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea045b7c-727a-4196-90bd-efa843f09ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
