{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981dc8da-02e1-40f8-b00a-cff25a652265",
   "metadata": {},
   "source": [
    "# WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fbe163-3c09-4241-9e87-8f929faef661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Ridge() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for Ridge: {'ridge__alpha': 0.1, 'ridge__solver': 'cholesky'}\n",
      "Ridge RMSE: 2.431924791782782\n",
      "Processing DecisionTreeRegressor() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for DecisionTreeRegressor: {'decisiontreeregressor__criterion': 'absolute_error', 'decisiontreeregressor__max_features': 3, 'decisiontreeregressor__min_samples_split': 5, 'decisiontreeregressor__splitter': 'best'}\n",
      "DecisionTreeRegressor RMSE: 2.6090191620789676\n",
      "Processing GradientBoostingRegressor() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for GradientBoostingRegressor: {'gradientboostingregressor__learning_rate': 0.001, 'gradientboostingregressor__loss': 'quantile', 'gradientboostingregressor__n_estimators': 25, 'gradientboostingregressor__warm_start': False}\n",
      "GradientBoostingRegressor RMSE: 4.081839488391592\n",
      "Processing RandomForestRegressor() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for RandomForestRegressor: {'randomforestregressor__criterion': 'squared_error', 'randomforestregressor__max_features': 3, 'randomforestregressor__min_samples_split': 5, 'randomforestregressor__n_estimators': 1}\n",
      "RandomForestRegressor RMSE: 2.888135543735479\n",
      "Processing AdaBoostRegressor() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for AdaBoostRegressor: {'adaboostregressor__learning_rate': 0.001, 'adaboostregressor__loss': 'linear', 'adaboostregressor__n_estimators': 1}\n",
      "AdaBoostRegressor RMSE: 1.1767923237950624\n",
      "Processing KNeighborsRegressor() for ./model_all_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for KNeighborsRegressor: {'kneighborsregressor__algorithm': 'ball_tree', 'kneighborsregressor__leaf_size': 5, 'kneighborsregressor__metric': 'euclidean', 'kneighborsregressor__n_neighbors': 5, 'kneighborsregressor__weights': 'distance'}\n",
      "KNeighborsRegressor RMSE: 3.1077912100364498\n",
      "Processing MLPRegressor(max_iter=1000) for ./model_all_data\\output_bfill_imputed.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 250\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Process each CSV file\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m--> 250\u001b[0m     best_model_info \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     best_models_info\u001b[38;5;241m.\u001b[39mappend(best_model_info)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Save the best model information for each CSV file to a CSV file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 207\u001b[0m, in \u001b[0;36mprocess_csv\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m param_grid:\n\u001b[0;32m    206\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid[model_name], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     best_estimator \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    209\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:753\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    737\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \n\u001b[0;32m    739\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:489\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_units\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# validate parameter weights\u001b[39;00m\n\u001b[0;32m    494\u001b[0m weights \u001b[38;5;241m=\u001b[39m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefs_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:533\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 533\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_grad_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_coef_inter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:282\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[1;32m--> 282\u001b[0m loss, coef_grads, intercept_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m grad \u001b[38;5;241m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:327\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    324\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Forward propagate\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Get loss\u001b[39;00m\n\u001b[0;32m    330\u001b[0m loss_func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:174\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Iterate over the hidden layers\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     activations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefs_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     activations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_[i]\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# For the hidden layers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\utils\\extmath.py:195\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 195\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m ):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\scipy\\sparse\\_base.py:1461\u001b[0m, in \u001b[0;36missparse\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[1;32m-> 1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array type?\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \n\u001b[0;32m   1464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01,],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 500],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"WS\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b999c-4aae-46cd-8af2-c3c6a4cc791d",
   "metadata": {},
   "source": [
    "# WS Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b27adc-a99f-40d7-940b-4487802b2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"WS_Benefit\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81bfd4-8b7d-4c50-b0e9-61a61217d0ef",
   "metadata": {},
   "source": [
    "# NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b29c4-16ac-43a9-8939-7232b728b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"NR\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7bb11-26ff-45af-9415-b6f58f84fd66",
   "metadata": {},
   "source": [
    "# NR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41144a-b0ee-4c84-a309-9321b69b65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"NR_Benefit\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f045c4-34c9-41a9-b1f3-d4521d91dcfc",
   "metadata": {},
   "source": [
    "# PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad59f7-4070-44ea-910b-269d9a6c9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"PR\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3abe42-6b55-4eba-91ee-f60baf5340f7",
   "metadata": {},
   "source": [
    "# PR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4b6a2-417a-4646-82f3-aaad183c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"PR_Benefit\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e6f50-a0b5-4589-803a-b389cd028dfe",
   "metadata": {},
   "source": [
    "# SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0b718-7bde-4f0c-b4c7-eba5fb5d3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"SR\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14e409-0cb4-4341-8c30-9918628afcea",
   "metadata": {},
   "source": [
    "# SR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969a3b2-2592-4493-a245-630e18d716c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"SR_Benefit\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bbc49-ac86-4249-9886-3ddb11e3a4e8",
   "metadata": {},
   "source": [
    "# SFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9f253-4b30-4704-8ac7-988792d03c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"SFST\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31bc889-6d9e-4f1e-b8e0-4c1b9ba3f30e",
   "metadata": {},
   "source": [
    "# SFST Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc7686-4685-40ee-97dc-dff3182ea232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge, LinearRegression, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF30', 'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'ridge__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridge__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'decisiontreeregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "        'decisiontreeregressor__min_samples_split': [1, 2, 3, 4, 5],\n",
    "        'decisiontreeregressor__max_features': [0, 1, 2, 3, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomforestregressor__n_estimators': [1, 50, 100],\n",
    "        'randomforestregressor__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'randomforestregressor__min_samples_split': [2, 5],\n",
    "        'randomforestregressor__max_features': [1, 3, 'sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        'gradientboostingregressor__learning_rate': [0.001, 0.01],\n",
    "        'gradientboostingregressor__n_estimators': [25, 50, 100],\n",
    "        'gradientboostingregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'adaboostregressor__n_estimators': [1, 20, 50, 100],\n",
    "        'adaboostregressor__learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0, 10],\n",
    "        'adaboostregressor__loss': ['linear', 'square', 'exponential']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'kneighborsregressor__n_neighbors': [2, 5, 10, 25],\n",
    "        'kneighborsregressor__weights': ['uniform', 'distance'],\n",
    "        'kneighborsregressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsregressor__leaf_size': [5, 30, 50],\n",
    "        'kneighborsregressor__metric': ['cityblock', 'cosine', 'euclidean', 'haversine', 'l1', 'l2', 'manhattan', 'nan_euclidean']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'elasticnet__l1_ratio': [0.25, 0.5, 0.75],\n",
    "        'elasticnet__fit_intercept': [True, False],\n",
    "        'elasticnet__precompute': [True, False],\n",
    "        'elasticnet__copy_X': [True, False],\n",
    "        'elasticnet__warm_start': [True, False],\n",
    "        'elasticnet__positive': [True, False],\n",
    "        'elasticnet__selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'SGDRegressor': {\n",
    "        'sgdregressor__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'sgdregressor__penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "        'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdregressor__warm_start': [True, False],\n",
    "    },\n",
    "    'SVR': {\n",
    "        'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'svr__degree': [1, 3, 5, 10],\n",
    "        'svr__gamma': ['scale', 'auto', 1.0, 5.0],\n",
    "        'svr__shrinking': [True, False]\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'bayesianridge__alpha_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__alpha_2': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_1': [1e-7, 1e-6, 1e-5],\n",
    "        'bayesianridge__lambda_2': [1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'kernelridge__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        'kernelridge__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "        'kernelridge__degree': [1, 2, 3, 5, 10],\n",
    "        'kernelridge__coef0': [0.0, 0.5, 1.0]\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'linearregression__fit_intercept': [True, False],\n",
    "        'linearregression__copy_X': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    },\n",
    "    'RANSACRegressor': {\n",
    "        'ransacregressor__min_samples': [None, 1, 2, 5, 10, 50],\n",
    "        'ransacregressor__max_trials': [1, 10, 50, 100, 150],\n",
    "        'ransacregressor__loss': ['absolute_error', 'squared_error']\n",
    "    },\n",
    "    'TheilSenRegressor': {\n",
    "        'theilsenregressor__max_subpopulation': [1, 10, 100, 1000],\n",
    "        'theilsenregressor__n_subsamples': [None, 1, 5, 10, 25],\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    Ridge(), DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor(), AdaBoostRegressor(),\n",
    "    KNeighborsRegressor(), MLPRegressor(max_iter=1000), ElasticNet(max_iter=1000), SGDRegressor(max_iter=1000),\n",
    "    SVR(cache_size=1000), BayesianRidge(max_iter=1000), KernelRidge(), LinearRegression(), RANSACRegressor(),\n",
    "    TheilSenRegressor()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"SFST_Benefit\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'rmse': float('inf')\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            rmse_tf = mean_squared_error(y_test, y_pred_tf, squared=False)\n",
    "            print(f\"TensorFlow RMSE: {rmse_tf}\")\n",
    "\n",
    "            if rmse_tf < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'rmse': rmse_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf.flatten(), 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            print(f\"{model_name} RMSE: {rmse}\")\n",
    "\n",
    "            if rmse < best_model_info['rmse']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('./model_all_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models\"+model_name+\"_info.csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a049f-5075-4a55-9ef8-e988af4cb6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97faf8a-29e9-4d5c-a30e-1f788cdc5bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b7730-7cfc-4078-9b64-04c07a5861d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
