\begin{enumerate}
    \item \textbf{PCA (Principal Component Analysis)}: A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

    \item \textbf{t-SNE (t-Distributed Stochastic Neighbor Embedding)}: A machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions.

    \item \textbf{UMAP (Uniform Manifold Approximation and Projection)}: A dimension reduction technique that can be used for visualization similarly to t-SNE, but also for general non-linear dimension reduction. It is based on manifold learning techniques and ideas from topological data analysis.

    \item \textbf{Isomap (Isometric Mapping)}: A non-linear dimensionality reduction method based on the geometric distances in the data, which is effective for datasets where nonlinear manifold structures are present.

    \item \textbf{LLE (Locally Linear Embedding)}: Another nonlinear dimension reduction technique that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs.

    \item \textbf{Truncated SVD (Singular Value Decomposition)}: An algorithmic approach for dimensionality reduction that uses truncated singular value decomposition. It is similar to PCA but suitable for sparse datasets.

    \item \textbf{ICA (Independent Component Analysis)}: A computational method for separating a multivariate signal into additive subcomponents that are maximally independent.

    \item \textbf{Kernel PCA}: An extension of PCA using techniques of kernel methods, which uses a kernel function to project dataset into a higher-dimensional space where linear separation is possible.

    \item \textbf{Gaussian Random Projection}: A simple and computationally efficient way to reduce dimensionality by projecting the original data into a randomly generated subspace of lower dimensionality using a Gaussian random matrix.
\end{enumerate}