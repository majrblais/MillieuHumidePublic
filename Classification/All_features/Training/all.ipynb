{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bd653-15a7-4014-9bf0-66ece9ba3bd4",
   "metadata": {},
   "source": [
    "# WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676b7730-7cfc-4078-9b64-04c07a5861d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RidgeClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for RidgeClassifier: {'ridgeclassifier__alpha': 0.1, 'ridgeclassifier__solver': 'auto'}\n",
      "RidgeClassifier Accuracy: 0.7142857142857143\n",
      "Processing DecisionTreeClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for DecisionTreeClassifier: {'decisiontreeclassifier__criterion': 'log_loss', 'decisiontreeclassifier__max_features': None, 'decisiontreeclassifier__min_samples_split': 4, 'decisiontreeclassifier__splitter': 'random'}\n",
      "DecisionTreeClassifier Accuracy: 0.9047619047619048\n",
      "Processing GradientBoostingClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for GradientBoostingClassifier: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__loss': 'log_loss', 'gradientboostingclassifier__n_estimators': 100, 'gradientboostingclassifier__warm_start': True}\n",
      "GradientBoostingClassifier Accuracy: 0.9523809523809523\n",
      "Processing RandomForestClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for RandomForestClassifier: {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_features': 'sqrt', 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 100}\n",
      "RandomForestClassifier Accuracy: 0.9047619047619048\n",
      "Processing AdaBoostClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for AdaBoostClassifier: {'adaboostclassifier__algorithm': 'SAMME', 'adaboostclassifier__learning_rate': 0.01, 'adaboostclassifier__n_estimators': 200}\n",
      "AdaBoostClassifier Accuracy: 0.8571428571428571\n",
      "Processing KNeighborsClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for KNeighborsClassifier: {'kneighborsclassifier__algorithm': 'auto', 'kneighborsclassifier__leaf_size': 30, 'kneighborsclassifier__metric': 'manhattan', 'kneighborsclassifier__n_neighbors': 10, 'kneighborsclassifier__weights': 'distance'}\n",
      "KNeighborsClassifier Accuracy: 0.8095238095238095\n",
      "Processing MLPClassifier(max_iter=1000) for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for MLPClassifier: {'mlpclassifier__activation': 'identity', 'mlpclassifier__hidden_layer_sizes': (100, 100, 100), 'mlpclassifier__learning_rate': 'adaptive', 'mlpclassifier__solver': 'sgd'}\n",
      "MLPClassifier Accuracy: 0.5714285714285714\n",
      "Processing LogisticRegression(max_iter=1000) for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for LogisticRegression: {'logisticregression__C': 0.1, 'logisticregression__max_iter': 200, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'saga'}\n",
      "LogisticRegression Accuracy: 0.9047619047619048\n",
      "Processing SGDClassifier() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for SGDClassifier: {'sgdclassifier__learning_rate': 'optimal', 'sgdclassifier__loss': 'hinge', 'sgdclassifier__penalty': 'l2', 'sgdclassifier__warm_start': True}\n",
      "SGDClassifier Accuracy: 0.7142857142857143\n",
      "Processing SVC() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for SVC: {'svc__C': 0.1, 'svc__degree': 1, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\n",
      "SVC Accuracy: 0.7619047619047619\n",
      "Processing GaussianNB() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for GaussianNB: {'gaussiannb__var_smoothing': 1e-09}\n",
      "GaussianNB Accuracy: 0.6190476190476191\n",
      "Processing LinearDiscriminantAnalysis() for ../All_data\\output_bfill_imputed.csv\n",
      "Best hyperparameters for LinearDiscriminantAnalysis: {'lineardiscriminantanalysis__shrinkage': 0.1, 'lineardiscriminantanalysis__solver': 'lsqr'}\n",
      "LinearDiscriminantAnalysis Accuracy: 0.8095238095238095\n",
      "Processing TensorFlow for ../All_data\\output_bfill_imputed.csv\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Accuracy: 0.6666666666666666\n",
      "Processing RidgeClassifier() for ../All_data\\output_custom_imputed.csv\n",
      "Best hyperparameters for RidgeClassifier: {'ridgeclassifier__alpha': 0.5, 'ridgeclassifier__solver': 'sag'}\n",
      "RidgeClassifier Accuracy: 0.6190476190476191\n",
      "Processing DecisionTreeClassifier() for ../All_data\\output_custom_imputed.csv\n",
      "Best hyperparameters for DecisionTreeClassifier: {'decisiontreeclassifier__criterion': 'gini', 'decisiontreeclassifier__max_features': None, 'decisiontreeclassifier__min_samples_split': 3, 'decisiontreeclassifier__splitter': 'random'}\n",
      "DecisionTreeClassifier Accuracy: 0.8095238095238095\n",
      "Processing GradientBoostingClassifier() for ../All_data\\output_custom_imputed.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 226\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Process each CSV file\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m--> 226\u001b[0m     best_model_info \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     best_models_info\u001b[38;5;241m.\u001b[39mappend(best_model_info)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Save the best model information for each CSV file to a CSV file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 183\u001b[0m, in \u001b[0;36mprocess_csv\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m param_grid:\n\u001b[0;32m    182\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid[model_name], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 183\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     best_estimator \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    185\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    607\u001b[0m         )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS']\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "model_directory = \"WS\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c5b9b-c1a9-429b-8303-fdd72671704a",
   "metadata": {},
   "source": [
    "# WS Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc17f84-593f-415a-b873-347d72667cb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS_Benefit']\n",
    "model_directory = \"WS_Benefit\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cfb824-0317-4020-8b3d-5d7ace67c0bd",
   "metadata": {},
   "source": [
    "# SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2b8fc-e982-48ab-ab12-7911c1c08c00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['SR']\n",
    "model_directory = \"SR\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc889b-4bed-45cd-a4b2-30caacef4380",
   "metadata": {},
   "source": [
    "# SR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284436b9-a4bd-4c5a-86ab-db70edc568f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['SR_Benefit']\n",
    "model_directory = \"SR_Benefit\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e68f3-cc65-49ac-84d1-0ef243ea9a67",
   "metadata": {},
   "source": [
    "# PR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f877d2-5412-4809-84bb-68b148ded5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['PR']\n",
    "model_directory = \"PR\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c5306-d356-4465-b592-8f61d471347c",
   "metadata": {},
   "source": [
    "# PR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2ccfe-00ec-40d4-8321-f547fce138da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['PR_Benefit']\n",
    "model_directory = \"PR_Benefit\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733ecc8-95a3-4d2d-9874-05162e9c5419",
   "metadata": {},
   "source": [
    "# NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423ae0a-a04f-4229-91e6-d68dcf97a179",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['NR']\n",
    "model_directory = \"NR\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b60f73-ccc0-4645-a47e-62b721c32cc5",
   "metadata": {},
   "source": [
    "# NR Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0c79e-1079-4815-a077-02344ede538b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['NR_Benefit']\n",
    "model_directory = \"NR_Benefit\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2bee1-78c0-4f9c-a891-921f08f565ef",
   "metadata": {},
   "source": [
    "# SFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e76d5-ea50-44dd-8e06-11c777d59a89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['SFST']\n",
    "model_directory = \"SFST\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23b861-2ba5-4ed3-be7f-c6e51df75adf",
   "metadata": {},
   "source": [
    "# SFST Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5c381-df46-4088-a08d-78046f904adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28',  'OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25',  'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41',  'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['SFST_Benefit']\n",
    "model_directory = \"SFST_Benefit\"\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'RidgeClassifier': {\n",
    "        'ridgeclassifier__alpha': [0.1, 0.5, 1.0],\n",
    "        'ridgeclassifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decisiontreeclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "        'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5],\n",
    "        'decisiontreeclassifier__max_features': [None, 'sqrt', 'log2']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "        'randomforestclassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradientboostingclassifier__loss': ['log_loss', 'deviance', 'exponential'],\n",
    "        'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "        'gradientboostingclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboostclassifier__n_estimators': [50, 100, 200],\n",
    "        'adaboostclassifier__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "        'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [5, 10, 15, 20],\n",
    "        'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "        'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'kneighborsclassifier__leaf_size': [30, 50, 70],\n",
    "        'kneighborsclassifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50, 50), (100, 100, 100), (100, 100, 100, 100)],\n",
    "        'mlpclassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpclassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'mlpclassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'logisticregression__C': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'logisticregression__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'sgdclassifier__warm_start': [True, False],\n",
    "    },\n",
    "    'SVC': {\n",
    "        'svc__C': [0.1, 1.0, 10.0],\n",
    "        'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'svc__degree': [1, 3, 5],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    RidgeClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier(), AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(), MLPClassifier(max_iter=1000), LogisticRegression(max_iter=1000), SGDClassifier(max_iter=1000),\n",
    "    SVC(), GaussianNB(), LinearDiscriminantAnalysis()\n",
    "]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Directory where you want to save your models\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    best_model_info = {\n",
    "        'csv_file': os.path.basename(file_path),\n",
    "        'model_name': None,\n",
    "        'hyperparameters': None,\n",
    "        'accuracy': 0\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models + ['TensorFlow']:  # Add TensorFlow model to the loop\n",
    "        print(f\"Processing {model} for {file_path}\")\n",
    "        if model == 'TensorFlow':\n",
    "            # Define the TensorFlow model\n",
    "            model_tf = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            # Compile the TensorFlow model\n",
    "            model_tf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Standardize the data for TensorFlow model\n",
    "            scaler_tf = StandardScaler()\n",
    "            X_train_scaled_tf = scaler_tf.fit_transform(X_train)\n",
    "            X_test_scaled_tf = scaler_tf.transform(X_test)\n",
    "\n",
    "            # Train the TensorFlow model\n",
    "            model_tf.fit(X_train_scaled_tf, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "            # Evaluate the TensorFlow model\n",
    "            y_pred_tf = model_tf.predict(X_test_scaled_tf)\n",
    "            y_pred_tf_classes = tf.argmax(y_pred_tf, axis=1).numpy()\n",
    "            accuracy_tf = accuracy_score(y_test, y_pred_tf_classes)\n",
    "            print(f\"TensorFlow Accuracy: {accuracy_tf}\")\n",
    "\n",
    "            if accuracy_tf > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': 'TensorFlow',\n",
    "                    'hyperparameters': None,\n",
    "                    'accuracy': accuracy_tf\n",
    "                })\n",
    "\n",
    "            # Save the TensorFlow model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_TensorFlow_model.h5\")\n",
    "            model_tf.save(model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred_tf_classes, 'Model': 'TensorFlow'}))\n",
    "        else:\n",
    "            model_name = model.__class__.__name__\n",
    "            pipeline = make_pipeline(StandardScaler(), model)\n",
    "            # Perform grid search for hyperparameters\n",
    "            if model_name in param_grid:\n",
    "                grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='accuracy')\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "                print(f\"Best hyperparameters for {model_name}: {best_params}\")\n",
    "            else:\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                best_estimator = pipeline\n",
    "                best_params = None\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "\n",
    "            if accuracy > best_model_info['accuracy']:\n",
    "                best_model_info.update({\n",
    "                    'model_name': model_name,\n",
    "                    'hyperparameters': best_params,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "\n",
    "            # Save the model\n",
    "            model_filename = os.path.join(model_directory, f\"{os.path.basename(file_path)}_{model_name}_model.pkl\")\n",
    "            joblib.dump(best_estimator, model_filename)\n",
    "\n",
    "            # Save the predictions and actual values\n",
    "            results.append(pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten(), 'Model': model_name}))\n",
    "\n",
    "    # Save the predictions and actual values to a CSV file\n",
    "    results_df = pd.concat(results, axis=0)\n",
    "    results_filename = f\"output_{os.path.basename(file_path)}_{results_columns[0]}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "    return best_model_info\n",
    "\n",
    "# Get the list of CSV files in the directory\n",
    "csv_files = glob.glob('../All_data/*.csv')\n",
    "\n",
    "# Initialize a list to store the best model information for each CSV file\n",
    "best_models_info = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    best_model_info = process_csv(csv_file)\n",
    "    best_models_info.append(best_model_info)\n",
    "\n",
    "# Save the best model information for each CSV file to a CSV file\n",
    "best_models_df = pd.DataFrame(best_models_info)\n",
    "best_models_df.to_csv(\"best_models_info\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "print(\"Best models information saved to best_models_info.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
