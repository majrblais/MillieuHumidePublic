{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b48c361-e281-4516-a65a-746c5c486513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HalfMultinomialLoss' object has no attribute 'get_init_raw_predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 178\u001b[0m\n\u001b[0;32m    175\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(best_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Perform feature selection and retraining for the best model\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m reduction_results, feature_selection_data, test_results \u001b[38;5;241m=\u001b[39m \u001b[43mload_evaluate_and_retrain_best_model_with_feature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Define X and y using the loaded data\u001b[39;00m\n\u001b[0;32m    181\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n",
      "Cell \u001b[1;32mIn[7], line 82\u001b[0m, in \u001b[0;36mload_evaluate_and_retrain_best_model_with_feature_selection\u001b[1;34m(csv_file, model_name, model_path, hyperparameters)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m---> 82\u001b[0m     y_pred_classes \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m accuracy_original \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_classes)\n\u001b[0;32m     85\u001b[0m selection_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_original}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\pipeline.py:515\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    514\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredict_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1293\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \n\u001b[0;32m   1281\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m     encoded_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39m_raw_prediction_to_decision(raw_predictions)\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(encoded_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1249\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \n\u001b[0;32m   1230\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m    array of shape (n_samples,).\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1247\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m )\n\u001b[1;32m-> 1249\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:685\u001b[0m, in \u001b[0;36mBaseGradientBoosting._raw_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_predict_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m     predict_stages(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, raw_predictions)\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\greystone\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:678\u001b[0m, in \u001b[0;36mBaseGradientBoosting._raw_predict_init\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    674\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    675\u001b[0m         shape\u001b[38;5;241m=\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39mK), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    676\u001b[0m     )\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 678\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_init_raw_predictions\u001b[49m(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_)\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    679\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    680\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HalfMultinomialLoss' object has no attribute 'get_init_raw_predictions'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Define the data columns and results columns\n",
    "data_columns = [\n",
    "    'OF2', 'OF3', 'OF4', 'OF5', 'OF6', 'OF7', 'OF8', 'OF9', 'OF10', 'OF11', 'OF13', 'OF14', 'OF15', 'OF16', 'OF17',\n",
    "    'OF18', 'OF19', 'OF20', 'OF21', 'OF22', 'OF23', 'OF24', 'OF25', 'OF26', 'OF27', 'OF28','OF31',\n",
    "    'OF33', 'OF34', 'OF37', 'OF38', 'F1', 'F2', 'F3_a', 'F3_b', 'F3_c', 'F3_d', 'F3_e', 'F3_f', 'F3_g', 'F4', 'F5', 'F6',\n",
    "    'F7', 'F8', 'F9', 'F10',  'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23',\n",
    "    'F24', 'F25', 'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40',\n",
    "    'F41', 'F43', 'F44', 'F45', 'F46', 'F47', 'F48', 'F49', 'F50', 'F51', 'F52', 'F53', 'F54', 'F55', 'F56', 'F57',\n",
    "    'F58', 'F59', 'F62', 'F63', 'F64', 'F65', 'F67', 'F68', 'S1', 'S2', 'S4', 'S5'\n",
    "]\n",
    "\n",
    "results_columns = ['WS']\n",
    "best_models_df = pd.read_csv(\"../Training/Results/best_models_infoWS.csv\")\n",
    "model_directory = \"../Training/Results/WS\"\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "feature_selection_techniques = {\n",
    "    \"SelectKBest_f_classif\": SelectKBest(score_func=f_classif),\n",
    "    \"SelectKBest_mutual_info_classif\": SelectKBest(score_func=mutual_info_classif),\n",
    "    \"VarianceThreshold\": VarianceThreshold(threshold=0.1),  # Example threshold, adjust as needed\n",
    "}\n",
    "\n",
    "# Define a mapping from model names to model classes\n",
    "model_mapping = {\n",
    "    'RidgeClassifier': RidgeClassifier,\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier,\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier,\n",
    "    'RandomForestClassifier': RandomForestClassifier,\n",
    "    'AdaBoostClassifier': AdaBoostClassifier,\n",
    "    'KNeighborsClassifier': KNeighborsClassifier,\n",
    "    'MLPClassifier': MLPClassifier,\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'SGDClassifier': SGDClassifier,\n",
    "    'SVC': SVC,\n",
    "    'Perceptron': Perceptron,\n",
    "    'PassiveAggressiveClassifier': PassiveAggressiveClassifier\n",
    "}\n",
    "\n",
    "# Function to load, evaluate, and retrain the best model with feature selection\n",
    "def load_evaluate_and_retrain_best_model_with_feature_selection(csv_file, model_name, model_path, hyperparameters):\n",
    "    # Load data from CSV\n",
    "    data = pd.read_csv(csv_file)\n",
    "    X = data[data_columns]\n",
    "    y = data[results_columns[0]]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Load the original model and calculate accuracy\n",
    "    if model_name == 'TensorFlow':\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    else:\n",
    "        model = joblib.load(model_path)\n",
    "        y_pred_classes = model.predict(X_test_scaled)\n",
    "\n",
    "    accuracy_original = accuracy_score(y_test, y_pred_classes)\n",
    "    selection_results = {'Original': accuracy_original}\n",
    "    feature_selection_data = []  # To store feature selection results\n",
    "    test_results = []  # To store test results (actual and predicted)\n",
    "\n",
    "    # Iterate over each feature selection technique\n",
    "    for name, selector in feature_selection_techniques.items():\n",
    "        print(f\"Applying {name} feature selection\")\n",
    "        accuracy_values = []\n",
    "\n",
    "        for k in range(2, len(data_columns) + 1):  # Iterate over all possible numbers of features\n",
    "            if isinstance(selector, SelectKBest):\n",
    "                selector.set_params(k=k)\n",
    "                X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "                X_test_selected = selector.transform(X_test)\n",
    "            elif isinstance(selector, VarianceThreshold):\n",
    "                # VarianceThreshold does not support `k` parameter, handle it separately\n",
    "                selector.set_params(threshold=0.1)\n",
    "                X_train_selected = selector.fit_transform(X_train, y_train)[:, :k]\n",
    "                X_test_selected = selector.transform(X_test)[:, :k]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Retrain the model with the selected features\n",
    "            if model_name == 'TensorFlow':\n",
    "                # Model setup and training for TensorFlow\n",
    "                model = tf.keras.Sequential([\n",
    "                    tf.keras.layers.InputLayer(input_shape=(X_train_selected.shape[1],)),\n",
    "                    tf.keras.layers.Dense(hyperparameters['units'], activation=hyperparameters['activation']),\n",
    "                    tf.keras.layers.Dense(3, activation='softmax')\n",
    "                ])\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                model.fit(X_train_selected, y_train, epochs=hyperparameters['epochs'], verbose=0)\n",
    "                y_pred = model.predict(X_test_selected)\n",
    "                y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            else:\n",
    "                # Model setup and training for other models\n",
    "                model_class = model_mapping.get(model_name)\n",
    "                if not model_class:\n",
    "                    print(f\"Unknown model name: {model_name}\")\n",
    "                    continue\n",
    "               \n",
    "                model_hyperparameters = {k.split('__', 1)[1]: v for k, v in hyperparameters.items() if\n",
    "                                         k.startswith(model_name.lower())}\n",
    "                model = model_class(**model_hyperparameters)\n",
    "                model.fit(X_train_selected, y_train)\n",
    "                y_pred_classes = model.predict(X_test_selected)\n",
    "\n",
    "            # Calculate accuracy and store the results\n",
    "            accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "            accuracy_values.append(accuracy)\n",
    "\n",
    "            # Store feature selection results\n",
    "            if isinstance(selector, SelectKBest):\n",
    "                selected_features = selector.get_support(indices=True)\n",
    "            elif isinstance(selector, VarianceThreshold):\n",
    "                selected_features = selector.get_support(indices=True)\n",
    "            feature_selection_data.append({\n",
    "                'CSV File': csv_file,\n",
    "                'Model': model_name,\n",
    "                'Selection Method': name,\n",
    "                'Number of Features': k,\n",
    "                'Accuracy': accuracy,\n",
    "                'Selected Features': [data_columns[i] for i in selected_features]\n",
    "            })\n",
    "\n",
    "            # Store test results\n",
    "            test_results.append({\n",
    "                'CSV File': csv_file,\n",
    "                'Model': model_name,\n",
    "                'Selection Method': name,\n",
    "                'Number of Features': k,\n",
    "                'Actual': ','.join(map(str, y_test.values)),\n",
    "                'Predicted': ','.join(map(str, y_pred_classes)),\n",
    "                'Accuracy': accuracy,\n",
    "                'Selected Features': [data_columns[i] for i in selected_features]\n",
    "            })\n",
    "\n",
    "        selection_results[name] = accuracy_values\n",
    "\n",
    "    return selection_results, feature_selection_data, test_results\n",
    "\n",
    "# Find the best model from best_models_df\n",
    "best_model = best_models_df.loc[best_models_df['accuracy'].idxmax()]\n",
    "\n",
    "# Load data for the best model\n",
    "csv_file = os.path.join('../Training/All_Data', best_model['csv_file'])\n",
    "model_name = best_model['model_name']\n",
    "model_path = os.path.join(model_directory,\n",
    "                          f\"{best_model['csv_file']}_{model_name}_model.pkl\" if model_name != 'TensorFlow' else f\"{best_model['csv_file']}_TensorFlow_model.h5\")\n",
    "hyperparameters = ast.literal_eval(best_model['hyperparameters'])\n",
    "\n",
    "# Perform feature selection and retraining for the best model\n",
    "reduction_results, feature_selection_data, test_results = load_evaluate_and_retrain_best_model_with_feature_selection(csv_file, model_name, model_path, hyperparameters)\n",
    "\n",
    "# Define X and y using the loaded data\n",
    "data = pd.read_csv(csv_file)\n",
    "X = data[data_columns]\n",
    "y = data[results_columns[0]]\n",
    "\n",
    "# Plot accuracy values for each reduction method for the current CSV file\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, accuracy_values in reduction_results.items():\n",
    "    if name != 'Original':  # Skip 'Original' since it doesn't have varying components\n",
    "        plt.plot(range(2, len(accuracy_values) + 2), accuracy_values, label=name, marker='o')\n",
    "\n",
    "# Add original accuracy to the plot\n",
    "plt.axhline(y=reduction_results['Original'], color='gray', linestyle='--', label='Original')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with Feature Selection for Model: {model_name} {results_columns[0]} ({best_model[\"csv_file\"]})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"feature_selection_accuracy_plot_{best_model['csv_file']}_{model_name}_{results_columns[0]}.png\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Save feature selection results to a CSV file\n",
    "feature_selection_df = pd.DataFrame(feature_selection_data)\n",
    "feature_selection_df.to_csv(\"feature_selection_results_\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "# Save test results to a CSV file\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df.to_csv(\"test_results_\"+results_columns[0]+\".csv\", index=False)\n",
    "\n",
    "# Find the method with the highest accuracy and lowest number of features\n",
    "best_method = min(feature_selection_data, key=lambda x: (-x['Accuracy'], x['Number of Features']))\n",
    "print(\"Method with the highest accuracy and lowest number of features:\")\n",
    "print(best_method)\n",
    "\n",
    "# Find the method with the highest accuracy proportionate to the number of features\n",
    "best_method_proportionate = max(feature_selection_data, key=lambda x: (x['Accuracy'] / x['Number of Features']))\n",
    "print(\"Method with the highest accuracy proportionate to the number of features:\")\n",
    "print(best_method_proportionate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f40664e-bc2e-4a0d-a258-299c9e956832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\prime\\anaconda3\\envs\\greystone\\lib\\site-packages (1.3.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\prime\\anaconda3\\envs\\greystone\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\prime\\anaconda3\\envs\\greystone\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\prime\\anaconda3\\envs\\greystone\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\prime\\anaconda3\\envs\\greystone\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Downloading scikit_learn-1.5.1-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB 682.7 kB/s eta 0:00:17\n",
      "   ---------------------------------------- 0.1/11.0 MB 825.8 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.3/11.0 MB 2.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/11.0 MB 3.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/11.0 MB 4.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/11.0 MB 5.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.1/11.0 MB 6.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.5/11.0 MB 6.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.6/11.0 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/11.0 MB 7.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/11.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/11.0 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.0/11.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.0 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 14.2 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.2\n",
      "    Uninstalling scikit-learn-1.3.2:\n",
      "      Successfully uninstalled scikit-learn-1.3.2\n",
      "Successfully installed scikit-learn-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\prime\\anaconda3\\envs\\greystone\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536d98d-a2c5-4511-9d6e-f0778163d5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
